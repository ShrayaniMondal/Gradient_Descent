{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import math\n",
    "from scipy.special import expit\n",
    "from scipy.special import softmax as smax\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the MNIST dataset from sklearn datasets module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = datasets.fetch_openml(\"mnist_784\", version=1, return_X_y = True)\n",
    "pixel_values, targets = data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualiszing the first digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label associated with it = 5\n"
     ]
    }
   ],
   "source": [
    "#visualizing one single digit\n",
    "single_digit = pixel_values[0,:].reshape(28,28)\n",
    "plt.imshow(single_digit, cmap='gray')\n",
    "plt.show()\n",
    "#the corresponing true label\n",
    "print(\"True Label associated with it =\",targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000, 10)\n"
     ]
    }
   ],
   "source": [
    "#since targets are of string type, we convert them into integers\n",
    "y_int = targets.astype(int)\n",
    "#now converting to one hot encoded arrays\n",
    "y = to_categorical(y_int)\n",
    "\n",
    "#normalize  and convert type to float 32\n",
    "x = (pixel_values/255)\n",
    "print(x.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting into training, validation, and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape (784, 60536)\n",
      "Y train shape (10, 60536)\n",
      "X val shape (784, 5264)\n",
      "Y val shape (10, 5264)\n",
      "X test shape (784, 4200)\n",
      "Y test shape (10, 4200)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.06, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.08, random_state=42)\n",
    "\n",
    "#reshaping\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "x_val = x_val.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "y_val = y_val.T\n",
    "\n",
    "\n",
    "print(\"X train shape\",x_train.shape)\n",
    "print(\"Y train shape\",y_train.shape)\n",
    "print(\"X val shape\",x_val.shape)\n",
    "print(\"Y val shape\",y_val.shape)\n",
    "print(\"X test shape\",x_test.shape)\n",
    "print(\"Y test shape\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Deep Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork():\n",
    "    def __init__(self, sizes, epochs=10, l_rate=0.05, gamma=0.9, batch_size = 64):\n",
    "        self.sizes = sizes\n",
    "        self.epochs = epochs\n",
    "        self.l_rate = l_rate\n",
    "        self.gamma = gamma \n",
    "        self.batch_size = batch_size \n",
    "        \n",
    "        # we save all parameters in the neural network in this dictionary\n",
    "        self.params, self.momenta = self.initialization()\n",
    "\n",
    "    def initialization(self):\n",
    "        # number of nodes in each layer\n",
    "        input_layer=self.sizes[0]\n",
    "        hidden_layer1=self.sizes[1]\n",
    "        hidden_layer2=self.sizes[2]\n",
    "        output_layer=self.sizes[3]\n",
    "\n",
    "        params = {\n",
    "            \"W1\" : np.random.randn(hidden_layer1, input_layer)*np.sqrt(1. / hidden_layer1),\n",
    "            \"W2\" : np.random.randn(hidden_layer2, hidden_layer1)*np.sqrt(1. / hidden_layer2),\n",
    "            \"W3\" : np.random.randn(output_layer, hidden_layer2)*np.sqrt(1. / output_layer),\n",
    "            \"B1\" : np.zeros((hidden_layer1,1), np.float32),\n",
    "            \"B2\" : np.zeros((hidden_layer2,1), np.float32),\n",
    "            \"B3\" : np.zeros((output_layer,1), np.float32),\n",
    "        }\n",
    "        \n",
    "        momenta = {\n",
    "            \"W1\" : np.zeros(params[\"W1\"].shape),\n",
    "            \"W2\" : np.zeros(params[\"W2\"].shape),\n",
    "            \"W3\" : np.zeros(params[\"W3\"].shape),\n",
    "            \"B1\" : np.zeros(params[\"B1\"].shape),\n",
    "            \"B2\" : np.zeros(params[\"B2\"].shape),\n",
    "            \"B3\" : np.zeros(params[\"B3\"].shape)\n",
    "        }\n",
    "        return params, momenta\n",
    "    \n",
    "    def compute_loss(self, y, y_hat):\n",
    "        L_sum = np.sum(np.multiply(y, np.log(y_hat)))\n",
    "        m = y.shape[1]\n",
    "        L = -(1./m) * L_sum\n",
    "        return L\n",
    "    \n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        #using the expit function from scipy\n",
    "        if derivative:\n",
    "            return  (self.sigmoid(x,False))*(1 - self.sigmoid(x,False))\n",
    "        return expit(x)\n",
    "    \n",
    "    def tanh(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            tanh_x = self.tanh(x)\n",
    "            return (1-tanh_x*tanh_x)\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        return smax(x, axis=0)\n",
    "        #return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "    def forward_pass(self, x_train):\n",
    "        params = self.params\n",
    "        \n",
    "        #for layer 1 to l-1, i.e., till one but last layer, get the outputs\n",
    "        #h0 is the input itself\n",
    "        params[\"H0\"] = x_train\n",
    "        \n",
    "        # input layer to hidden layer 1\n",
    "        params[\"A1\"] = np.matmul(params[\"W1\"], params[\"H0\"]) + params[\"B1\"]\n",
    "        params[\"H1\"] = self.sigmoid(params[\"A1\"])\n",
    "\n",
    "        # hidden layer 1 to hidden layer 2\n",
    "        params[\"A2\"] = np.matmul(params[\"W2\"], params[\"H1\"]) + params[\"B2\"]\n",
    "        params[\"H2\"] = self.sigmoid(params[\"A2\"])\n",
    "\n",
    "        # hidden layer 2 to output layer\n",
    "        params[\"A3\"] = np.matmul(params[\"W3\"], params[\"H2\"]) + params[\"B3\"]\n",
    "        params[\"H3\"] = self.softmax(params[\"A3\"])\n",
    "                \n",
    "        #return the predicted out that is H3\n",
    "        return params[\"H3\"]\n",
    "    \n",
    "    def backward_pass(self, y_train, output):\n",
    "        \n",
    "        #Back Propagation to learn the weights and biases\n",
    "        #we will get the output from the forward pass\n",
    "        params = self.params\n",
    "        change_w = {}\n",
    "        n_samples = y_train.shape[1]\n",
    "        \n",
    "        # Calculate W3 update\n",
    "        error = output - y_train\n",
    "        change_w['W3'] = (1./n_samples) * np.matmul(error, params[\"H2\"].T)\n",
    "        change_w['B3'] = (1./n_samples) * np.sum(error,axis=1, keepdims=True)\n",
    "\n",
    "        # Calculate W2 update\n",
    "        # hadamard operation between : [compute the graients wrt to the layer below],[derivative of activation function]\n",
    "        error = np.multiply(np.matmul(params[\"W3\"].T,error), self.sigmoid(params[\"A2\"], derivative=True))\n",
    "        change_w['W2'] = (1./n_samples) * np.matmul(error, params[\"H1\"].T)\n",
    "        change_w['B2'] = (1./n_samples) * np.sum(error,axis=1, keepdims=True)\n",
    "\n",
    "        # Calculate W1 update\n",
    "        # hadamard operation between : [compute the graients wrt to the layer below],[derivative of activation function]\n",
    "        error = np.multiply(np.matmul(params[\"W2\"].T,error), self.sigmoid(params[\"A1\"], derivative=True))\n",
    "        change_w['W1'] = (1./n_samples) * np.matmul(error, params[\"H0\"].T)\n",
    "        change_w['B1'] = (1./n_samples) * np.sum(error,axis=1, keepdims=True)\n",
    "        \n",
    "        return change_w\n",
    "    \n",
    "    def update_network_parameters(self, changes_to_w):\n",
    "        for key, value in changes_to_w.items():\n",
    "            self.momenta[key] = (self.l_rate * value) + (self.gamma * self.momenta[key])\n",
    "            self.params[key] = self.params[key] - self.momenta[key]\n",
    "    \n",
    "    def get_lookahead_parameters(self):\n",
    "        params_og = {}\n",
    "        for key, value in self.momenta.items():\n",
    "            params_og[key] = self.params[key]\n",
    "            self.params[key] = self.params[key] - (self.gamma * self.momenta[key])\n",
    "        return params_og\n",
    "    \n",
    "    def get_og_params(self, params_og):\n",
    "        for key, value in params_og.items():\n",
    "            self.params[key] = value\n",
    "    \n",
    "    def compute_accuracy(self, x_val, y_val):\n",
    "        output = self.forward_pass(x_val)\n",
    "        pred = np.argmax(output,axis=0)\n",
    "        actual = np.argmax(y_val,axis=0)\n",
    "        predictions = (actual == pred)\n",
    "        return np.average(predictions)\n",
    "    \n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        start_time = time.time()\n",
    "        for iteration in range(self.epochs):\n",
    "            \n",
    "            batches = math.ceil(x_train.shape[1]/self.batch_size)\n",
    "            for j in range(batches):\n",
    "                begin = j * self.batch_size\n",
    "                end = min(begin + self.batch_size, x_train.shape[1])\n",
    "                \n",
    "                x_mini = x_train[:, begin:end]\n",
    "                y_mini = y_train[:, begin:end]\n",
    "                \n",
    "                params_og = self.get_lookahead_parameters()\n",
    "                \n",
    "                output = self.forward_pass(x_mini)\n",
    "            \n",
    "                changes_to_w = self.backward_pass(y_mini, output)\n",
    "                \n",
    "                self.get_og_params(params_og) #setting the weights and biases to original\n",
    "                \n",
    "                #update after one pass on a mini batch\n",
    "                self.update_network_parameters(changes_to_w)\n",
    "            \n",
    "            train_ouput = self.forward_pass(x_train)\n",
    "            train_cost = self.compute_loss(y_train, train_ouput)\n",
    "            val_ouput = self.forward_pass(x_val)\n",
    "            val_cost = self.compute_loss(y_val, val_ouput)\n",
    "            print(\"Epoch {}: Training cost = {}, Validation cost = {}\".format(iteration+1 ,train_cost, val_cost))\n",
    "        end_time = time.time()\n",
    "        print(\"Total time taken for {} epochs is {}\".format(self.epochs,end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training cost = 0.25618417329028054, Validation cost = 0.25824970474547365\n",
      "Epoch 2: Training cost = 0.17943576921392007, Validation cost = 0.183827598518314\n",
      "Epoch 3: Training cost = 0.13637107621970326, Validation cost = 0.14523172175979998\n",
      "Epoch 4: Training cost = 0.10811081669030358, Validation cost = 0.12190651276886957\n",
      "Epoch 5: Training cost = 0.08844926238702663, Validation cost = 0.10719557705689702\n",
      "Epoch 6: Training cost = 0.0740042253775178, Validation cost = 0.09742874064688778\n",
      "Epoch 7: Training cost = 0.06293710871548264, Validation cost = 0.09064372647377829\n",
      "Epoch 8: Training cost = 0.05409306303277052, Validation cost = 0.08569738258968915\n",
      "Epoch 9: Training cost = 0.04684126706891818, Validation cost = 0.08199481082660712\n",
      "Epoch 10: Training cost = 0.040836998935727085, Validation cost = 0.07924192082531366\n",
      "Total time taken for 10 epochs is 29.900261163711548\n"
     ]
    }
   ],
   "source": [
    "#training and validating\n",
    "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 10])\n",
    "dnn.train(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[416   0   0   1   1   0   3   0   1   1]\n",
      " [  0 458   0   1   1   0   0   2   4   3]\n",
      " [  1   0 388   6   1   0   1   6   1   0]\n",
      " [  0   0   1 451   1   5   0   1   6   2]\n",
      " [  0   0   3   0 339   0   1   2   0   2]\n",
      " [  0   0   0   4   1 382   5   0   0   1]\n",
      " [  2   0   3   0   0   4 405   1   2   0]\n",
      " [  0   1   2   3   1   0   0 419   1   3]\n",
      " [  0   1   2   3   2   5   2   2 406   4]\n",
      " [  0   0   0   1   6   0   0   2   1 413]]\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       423\n",
      "           1       1.00      0.98      0.99       469\n",
      "           2       0.97      0.96      0.97       404\n",
      "           3       0.96      0.97      0.96       467\n",
      "           4       0.96      0.98      0.97       347\n",
      "           5       0.96      0.97      0.97       393\n",
      "           6       0.97      0.97      0.97       417\n",
      "           7       0.96      0.97      0.97       430\n",
      "           8       0.96      0.95      0.96       427\n",
      "           9       0.96      0.98      0.97       423\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      4200\n",
      "   macro avg       0.97      0.97      0.97      4200\n",
      "weighted avg       0.97      0.97      0.97      4200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_output = dnn.forward_pass(x_test)\n",
    "\n",
    "predictions = np.argmax(test_output, axis=0)\n",
    "labels = np.argmax(y_test, axis=0)\n",
    "\n",
    "print(confusion_matrix(predictions, labels))\n",
    "print(\"\\n\\n\")\n",
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set = 0.9707142857142858\n"
     ]
    }
   ],
   "source": [
    "testing_accuracy = dnn.compute_accuracy(x_test,y_test)\n",
    "print(\"Accuracy on Test Set =\",testing_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
